---
title: "Example R Markdown Notebook"
output:
  html_document:
    df_print: paged
jupyter:
  kernelspec:
    display_name: R
    language: R
    name: ir
  language_info:
    codemirror_mode: r
    file_extension: .r
    mimetype: text/x-r-source
    name: R
    pygments_lexer: r
    version: 3.4.3
---

## NLE Digital Newspaper Collection (DEA)

This notebook presents a pilot workflow on accessing digitized newspaper data in the National Library of Estonia. The collection currently includes 386,950 issues from 2200 periodical publications (including journals, magazines and newspapers). This amounts to a total of 3,635,503 pages of text and 6,298,369 articles segmented from this. Some of these texts are free to access, others need to follow some access restrictions due to copyright and other legal requirements. This notebook connects to a subsample of freely accessible texts, amounting to ~1,000,000 articles. 

NB! These texts have the licence: Free access - restricted use. This publication's copyright protection has expired but the rights of works contained in the publication may be protected. The works may be used for private purposes or study and research purposes. In other cases please ascertain that the copyright term has expired.

### Usage

The Notebook runs R-3.6.0 on the Jupyter Notebook, so regular R commands will work. To run a block in Jupyter Notebook, select the block and click on he the > 'run' icon above. To run a block in RStudio find the green > on top of the block.


### Required libraries

```{r, results=F, warning=F,messages=F}
library(data.table)
library(tidyverse)
library(lubridate)
library(plotly)
library(utils)
library(DT)

```


### Reading the files

This repository contains an index for the DEA periodical collection. `all_issues_all.tsv.zip` contains metainformation on each of the 383,388 issues of 2200 periodicals currently digitally available in the National Library of Estonia. 


```{r}

# Read in the metadata file for all issues in the collection
all_issues <- fread("unzip -p all_issues.tsv.zip")

# Add some new variables
all_issues[,firstyear:=min(year,na.rm=T),.(series)]
all_issues[,lastyear:=max(year,na.rm=T),by=.(series)]
all_issues[,title:=str_remove(DocumentTitle,";.*")]
all_issues[,number_of_issues:=.N,.(series)]


```


### Overview of the publications

The following block prints an overview of the metadata in an interactive table. The interactive table can be accessed below, or from the link [here](summarytable.html).

```{r}


summary_allissues <- unique(all_issues[,.SD[1],.(series)][,.(title,series,firstyear,lastyear,number_of_issues)])

#In Rstudio, run this line instead
#datatable(summary_allissues, filter = list(position = 'top', clear = FALSE),options = list(autoWidth = FALSE,   columnDefs = list(list(width = '200px', targets = c(1)),list(width = '200px', targets = c(2)),list(width = '50px', targets = c(2)),list(width = '50px', targets = c(2))),scrollX=T,     pageLength = 25))

DT::saveWidget(datatable(summary_allissues, filter = list(position = 'top', clear = FALSE),options = list(autoWidth = FALSE,
  columnDefs = list(list(width = '200px', targets = c(1)),list(width = '200px', targets = c(2)),list(width = '50px', targets = c(2)),list(width = '50px', targets = c(2))),scrollX=T,
    pageLength = 25)),"foo.html")
IRdisplay::display_html('<iframe src="summarytable.html" width=100%, height=1000></iframe> ')

```


### Graphic overviews of the collection

The number of issues over time in the collection.

```{r}
all_issues[,.N,year] %>% 
   ggplot(aes(x=year,y=N))+
   geom_col()
```

The issues colored by publication type.

```{r}
all_issues[,.N,.(year,DocumentType)] %>% 
   ggplot(aes(x=year,y=N, fill=DocumentType))+
   geom_col()
```

The issues with largest number of articles.

```{r}

toplist <- all_issues[,.N,series][order(-N)][1:20]
toplist[,series:=factor(series, levels=unique(series))]

toplist %>% 
   ggplot(aes(x=series,y=N, fill=series))+
   geom_col()+
   coord_flip()
```

The publication spans of the 20 publications with the largest collections.

```{r}

durations <- unique(all_issues[,.(series,firstyear,lastyear)])[toplist,on="series"][order(firstyear)]
durations[,series:=factor(series, levels=unique(series))]

durations %>% 
   ggplot(aes(y=series,color=series))+
   geom_segment(aes(yend=series,x=firstyear,xend=lastyear),size=3)
   coord_flip()

```


### Access and licences of the files

```{r}

#Metainformation on the publications, with licences and access rights of the files, and other data.
license_overview_plus_meta <- fread("unzip -p license_overview_plus_meta.tsv.zip")
all_issues_plus_access <- merge(all_issues,license_overview_plus_meta[,.(licence,language,country,section,keyid)],by.x="series",by.y="keyid")


#Color is misleading on this one, embargo is usually short term and most of it is open for mining.
all_issues_plus_access <- all_issues_plus_access[order(licence)]
all_issues_plus_access[,series:=factor(series,levels=unique(series))]
p<- all_issues_plus_access[,.N,.(year,series,licence)] %>% 
  ggplot(aes(x=year,y=N,text=series, fill=licence))+
  geom_col()
ggplotly(p)

htmlwidgets::saveWidget(as_widget(ggplotly(p)), "graph1.html")


IRdisplay::display_html('<iframe src="graph1.html" width=100%, height=1000></iframe> ')
```

```{r}

# Languages in the collection
all_issues_plus_access <- all_issues_plus_access[order(language)]
all_issues_plus_access[,series:=factor(series,levels=unique(series))]
p<- all_issues_plus_access[,.N,.(year,series,language)] %>% 
  ggplot(aes(x=year,y=N,text=series, fill=language))+
  geom_col()
ggplotly(p)


```

```{r}

# Countries of publication in the collection
all_issues_plus_access <- all_issues_plus_access[order(country)]
all_issues_plus_access[,series:=factor(series,levels=unique(series))]
p<- all_issues_plus_access[,.N,.(year,series,country)] %>% 
  ggplot(aes(x=year,y=N,text=series, fill=country))+
  geom_col()
ggplotly(p)


```


```{r}

# Genre of the publication
all_issues_plus_access <- all_issues_plus_access[order(section)]
all_issues_plus_access[,series:=factor(series,levels=unique(series))]
p<- all_issues_plus_access[,.N,.(year,series,section)] %>% 
  ggplot(aes(x=year,y=N,text=series, fill=section))+
  geom_col()
ggplotly(p)

```


```{r}

# Overview of one genre in the dataset
p<- all_issues_plus_access[section=="Art, music, theatre, film"][,.N,.(year,series,section)] %>% 
  ggplot(aes(x=year,y=N,text=series, fill=series))+
  geom_col()
ggplotly(p)


```


```{r}


summary_all_plus_access <- unique(all_issues_plus_access[,.SD[1],.(series)][,.(title,series,firstyear,lastyear,number_of_issues, licence, section)])


#In Rstudio, run this line instead
#datatable(summary_allissues, filter = list(position = 'top', clear = FALSE),options = list(autoWidth = FALSE,   columnDefs = list(list(width = '200px', targets = c(1)),list(width = '200px', targets = c(2)),list(width = '50px', targets = c(2)),list(width = '50px', targets = c(2))),scrollX=T,     pageLength = 25))

DT::saveWidget(datatable(summary_all_plus_access, filter = list(position = 'top', clear = FALSE),options = list(autoWidth = FALSE,
  columnDefs = list(list(width = '200px', targets = c(1)),list(width = '200px', targets = c(2)),list(width = '50px', targets = c(2)),list(width = '50px', targets = c(2))),scrollX=T,
    pageLength = 25)),"foo.html")
IRdisplay::display_html('<iframe src="foo.html" width=100%, height=1000></iframe> ')

```


## Download texts from the collection


This code allows texts to be downloaded from the collection. Currently, the code links to a random sample of 1,000 collection files that contain a bit less than 1,000,000 articles altogether. In the repository is an index file `file_index_sample.tsv.zip`, this links the article unique ids to the .zip archives that they are placed in. With that file, we can take a subset of the collection, download only the files relevant to us, and process them after.

```{r}

# File index contains article unique id-s and the references to the collection files that they are in.
file_index <- fread("unzip -p file_index_sample.tsv.zip")
file_index[,DocumentID:=str_remove(filename,"\\..*")] # Get issue name from the article name.


# Let's build a subsample. Let's take all newspaper articles from the 1890s.
subset <- all_issues[str_detect(DocumentType,"NEWSPAPER")&year>1890&year<1900] #

# We can get the list of relevant files by merging it with the file index. Not all articles are available in the sample used in this workshop.
subset_w_fileinfo <- merge(subset,file_index, by="DocumentID")
#19000 articles in that subset...




```



```{r}

# We can look at an overview of the subset.
subset_w_fileinfo[,issues_subset:=.N,.(series)]
subset_w_fileinfo <- subset_w_fileinfo[,.SD[1],.(series)]
summary_subset <- unique(subset_w_fileinfo[,.(title,series,firstyear,lastyear,issues_subset)])
datatable(summary_subset)


```




### Preloading the relevant collection to the environment

The text files associated with this notebook are distributed into compressed .zip files that include up to 1,000 articles each. The directory structure retains the publication and date information, and the unique id is given also in the file. The article is placed on one line with html tags, and the unique id is given on the same line before it, separated by the tag. The files within the .zip archives also contain a watermark that these files originate the National Library of Estonia collections.


```{r}

preloadcollectionname <- "preloadedcollection1"
# We use the subset with fileinfo to get the list of relevant files, and download them to the virtual environment that we use.
requiredfiles <- subset_w_fileinfo[,unique(archive)]
dir.create(preloadcollectionname)
for (i in requiredfiles){
  system(paste0("wget --quiet data.digar.ee/text/",i," -O ",preloadcollectionname,"/",i))
}
  

  
```





### Let's download a bigger sample


```{r}

#Another possible sample
#This one takes all publications containing the word postimees (e.g. eestipostimees, parnupostimees, postimeesew) between 1850 and 1930.
subset <- all_issues[str_detect(series,"postimees")&year>1850&year<1930] #

subset_w_fileinfo <- merge(subset,file_index, by="DocumentID")

#p<- subset_w_fileinfo[series=="postimees"][,.N,.(year,series)] %>% 
#  ggplot(aes(x=year,y=N,text=series))+
#  geom_col()
#ggplotly(p)


```



```{r}

preloadcollectionname <- "preloadedcollection1"
# We use the subset with fileinfo to get the list of relevant files, and download them to the virtual environment that we use.
requiredfiles <- subset_w_fileinfo[,unique(archive)]
dir.create(preloadcollectionname)
for (i in requiredfiles){
  system(paste0("wget --quiet data.digar.ee/text/",i," -O ",preloadcollectionname,"/",i))
}
  

  
```




### Finding texts with particular keywords within the set


```{r}

# We can look for transportation devices within the dataset, e.g. mootorratas (motor bike), raudtee (railroad), aurulaew (steamboat), etc

# And we can look 

## One thing to note, 
searchterm <- "ratas|auru|raudtee|jalgratas|mootorratas"
filename <- "search1.txt"
preloadcollectionname <- "preloadedcollection1"

#Technically, each article is in a separate file and in a single row. The same row also contains the article unique identifier.
system(paste0("rm ",filename,"; for file in ", preloadcollectionname,"/*.zip; do unzip -c $file | grep -iE '",searchterm,"' >> ",filename,"; done"))

#Writing the query out on command line would look like this.
#system(paste0("rm search2.txt; for file in preloadedcollection/*.zip; do unzip -c $file | grep -i ' aurulaew' >> search2.txt; done"))
#system(paste0("rm search3.txt; for file in preloadedcollection/*.zip; do unzip -c $file | grep -i ' õnnetus' >> search3.txt; done"))

# You can also simply search for nothing, and get all the texts in the collection
#system(paste0("for file in preloadedcollection/*.zip; do unzip -c $file | grep -i ' ' >> alltexts.txt; done"))


```




```{r}

# Read the search results from file
filename <- "search1.txt"

# All the search results from our preloaded collection
texts <- fread(filename,header=F,sep="\t")

# Join all the texts with our chosen subset to get only the relevant results
# Here, only metainformation is included
texts[,V3:=str_remove(V1,"\\..*")]
subsetmetainfo<- merge(subset,texts[,.(V3)],by.x="DocumentID",by.y="V3")


```


## Download the texts

There are different formats you may want the texts in. An easy way is simply to download the search results file as the raw text corpus. If you want to add metainformation in a way that can be processed by excel, you can run the following code and get metainfo.tsv from this. The code will also write each of the texts as separate files, and zip them together so taht they can be easily downloaded.

```{r}

# Join all the texts with our chosen subset to get only the relevant results
# Here, full texts are included
subsetcollection <- merge(subset,texts,by.x="DocumentID",by.y="V3")

## Make the subset downloadable
collectionname <-  "download1"
dir.create(collectionname)
fwrite(subsetcollection,paste0(collectionname,"/metainfo.tsv"),sep="\t")

dir.create(paste0(collectionname,"/files"))
for (i in 1:nrow(texts)){
  write_file(subsetcollection[i,V2],path=paste0(collectionname,"/files/",texts[i,V1],".txt"))
}

fwrite(subsetcollection,paste0(collectionname,"/texts_plus_metainfo.tsv"),sep="\t")



files2zip <- dir(paste0(collectionname,'/files/'), full.names = TRUE)
zip(zipfile = paste0(collectionname,"/",collectionname,".zip"), files = files2zip)


```

## Simple text processing

Once we have the texts, we can do simple text processing on them.


```{r process the texts}

# Simple text processing in R.
library(tidytext)
# For some examples see: https://www.tidytextmining.com/


subsettexts_w_meta <- merge(subset,texts,by.x="DocumentID",by.y="V3")

tokens_w_meta <- subsettexts_w_meta %>% 
  unnest_tokens(word, V2) %>% 
  data.table()



```




```{r}


wordcounts <- tokens_w_meta[,.N,word][order(-N)]
#wordcounts[str_detect(word,"ratas")]
#wordcounts[str_detect(word,"auru")]
wordcounts[str_detect(word,searchterm)]



```





```{r}
#Count mentions
tokens_w_meta[str_detect(word,"jalgratas")][,.N,year][order(year)]

```



```{r}
#Plot mentions
tokens_w_meta[str_detect(word,"jalgratas")][,.N,.(year)] %>% 
  ggplot(aes(x=year,y=N))+
  geom_line()

```



```{r}
#Plot mentions
tokens_w_meta[str_detect(word,"raudtee")][,.N,.(year)] %>% 
  ggplot(aes(x=year,y=N))+
  geom_line()

```


```{r}
#Plot mentions
tokens_w_meta[str_detect(word,"jalgratas|raudtee|aurulaew")][,searchword:=str_extract(word,"jalgratas|raudtee|aurulaew")][,.N,.(searchword,year)] %>% 
  ggplot(aes(x=year,y=N,colour=searchword,group=searchword))+
  geom_line()

```



## We can also build a specialized collection for download

```{r}

# Let's build a subsample. Let's take all newspaper articles from the 1890s.
subset <- all_issues[str_detect(DocumentType,"NEWSPAPER")&year>1930&year<1940] #

#
# Join all the texts with our chosen subset to get only the relevant results
# Here, full texts are included
subsetcollection <- merge(subset,texts,by.x="DocumentID",by.y="V3")

## Make the subset downloadable
collectionname <-  "download2"
dir.create(collectionname)
fwrite(subsetcollection,paste0(collectionname,"/metainfo.tsv"),sep="\t")

dir.create(paste0(collectionname,"/files"))
for (i in 1:nrow(texts)){
  write_file(subsetcollection[i,V2],path=paste0(collectionname,"/files/",texts[i,V1],".txt"))
}

fwrite(subsetcollection,paste0(collectionname,"/texts_plus_metainfo.tsv"),sep="\t")



files2zip <- dir(paste0(collectionname,'/files/'), full.names = TRUE)
zip(zipfile = paste0(collectionname,"/",collectionname,".zip"), files = files2zip)


```

We can use external software like [AntConc](https://www.laurenceanthony.net/software/antconc/) to explore this. Let's download this and have a look.